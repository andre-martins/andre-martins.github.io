<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="generator" content="Pelican" />
        <title>Project Examples for Deep Structured Learning (Spring 2022)</title>
        <link rel="stylesheet" href="../theme/css/main.css" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="../">André F. T. Martins</a></h1>
                <nav><ul>
                    <li><a href="/index.html">Home</a></li>
                    <li><a href="/pages/jobs.html">Jobs</a></li>
                    <li><a href="/pages/publications.html">Publications</a></li>
                    <li><a href="/pages/software.html">Software</a></li>
                    <li><a href="/pages/courses.html">Courses</a></li>
                    <li><a href="/pages/sardine.html">SARDINE Lab</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
    <h1 class="entry-title">Project Examples for Deep Structured Learning (Spring 2022)</h1>
    
    <p>We suggest below some project ideas. Feel free to use this as inspiration for your project. Talk to us for more details.</p>
<hr />
<h1>Multimodal Deep Learning</h1>
<ul>
<li><strong>Problem:</strong> Many tasks require combining different modalities, such as language and vision, language and speech, etc. This project will survey some of these approaches.</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://visualqa.org/">Visual Question Answering</a>  </li>
<li><a href="https://arxiv.org/abs/2109.04448">S. Frank, E. Bugliarello, and D. Elliott. "Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers." EMNLP 2021.</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Deep Q-Learning for Text Generation</h1>
<ul>
<li><strong>Problem:</strong> Current models for text generation trained with maximum likelihood estimation often suffer from exposure bias. New metrics for text generation (such as COMET, BLEURT, BARTSCORE) offer new strategies to train systems by maximizing a better reward function, using reinformement learning techniques.  </li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://arxiv.org/abs/2106.07704">Han Guo, Bowen Tan, Zhengzhong Liu, Eric P. Xing, Zhiting Hu. "Text Generation with Efficient (Soft) Q-Learning"</a>  </li>
<li><a href="https://arxiv.org/abs/2104.05336">Rémi Leblond, Jean-Baptiste Alayrac, Laurent Sifre, Miruna Pislar, Jean-Baptiste Lespiau, Ioannis Antonoglou, Karen Simonyan, Oriol Vinyals. "Machine Translation Decoding beyond Beam Search"</a>  </li>
<li><a href="https://arxiv.org/abs/2112.08670">Richard Yuanzhe Pang, He He, Kyunghyun Cho. "Amortized Noisy Channel Neural Machine Translation."</a>  </li>
</ol>
</li>
</ul>
<hr />
<h1>Global Workspace Theory</h1>
<ul>
<li><strong>Problem:</strong> Current deep learning models lack higher order cognition capabilities. Global Workspace Theory is a cognitive science theory of consciousness that serves as inspiration to endow higher order cognition capabilities to neural networks.</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://arxiv.org/abs/2103.01197">Anirudh Goyal, Aniket Didolkar, Alex Lamb, Kartikeya Badola, Nan Rosemary Ke, Nasim Rahaman, Jonathan Binas, Charles Blundell, Michael Mozer, Yoshua Bengio. "Coordination Among Neural Modules Through a Shared Global Workspace."</a>  </li>
<li><a href="https://arxiv.org/abs/2011.15091">Anirudh Goyal, Yoshua Bengio. "Inductive Biases for Deep Learning of Higher-Level Cognition."</a>  </li>
</ol>
</li>
</ul>
<hr />
<h1>GFlowNets</h1>
<ul>
<li><strong>Problem:</strong> Survey Generative Flow Networks (GFlowNets), a recent method that allows samples a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function.</li>
<li><strong>References:</strong><ol>
<li><a href="https://arxiv.org/abs/2111.09266">Yoshua Bengio, Tristan Deleu, Edward J. Hu, Salem Lahlou, Mo Tiwari, Emmanuel Bengio. "GFlowNet Foundations."</a>  </li>
<li><a href="https://yoshuabengio.org/2022/03/05/generative-flow-networks/">Yoshua Bengio. Generative Flow Networks.</a>  </li>
</ol>
</li>
</ul>
<hr />
<h1>Prompting / Adaptors</h1>
<ul>
<li><strong>Problem:</strong> Large pretrained language models can be expensive to fine-tune. Lightweight strategies include adaptors and prompting methods. The goal is to survey and potentially experiment with some of these techniques.</li>
<li><strong>Data:</strong> See papers.</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://arxiv.org/abs/2107.13586">Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig. "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"</a> and references therein.</li>
</ol>
</li>
</ul>
<hr />
<h1>Uncertainty Quantification</h1>
<ul>
<li><strong>Problem:</strong> How to estimate the uncertainty of a classifier or regressor?</li>
<li><strong>Method:</strong> Monte Carlo dropout, deep ensembles, heteroscedastic regression, direct epistemic uncertainty prediction, etc.</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://arxiv.org/abs/2102.08501">Moksh Jain, Salem Lahlou, Hadi Nekoei, Victor Butoi, Paul Bertin, Jarrid Rector-Brooks, Maksym Korablyov, Yoshua Bengio. "DEUP: Direct Epistemic Uncertainty Prediction."</a>  </li>
<li><a href="https://arxiv.org/abs/1703.04977">Alex Kendall, Yarin Gal. "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?" NeurIPS 2017.</a>  </li>
</ol>
</li>
</ul>
<hr />
<h1>Sub-quadratic Sequence Models</h1>
<ul>
<li><strong>Problem:</strong> Transformers and BERT models are extremely large and expensive to train and keep in memory. The goal of this project is to survey or to make Transformers more efficient in terms of time and memory complexity by reducing the quadratic cost of self-attention or by inducing a sparser and smaller model. One possibility is to experiment with the recently proposed S4 model [1,2].</li>
<li><strong>Method:</strong> See references below.</li>
<li><strong>Data:</strong> Wikitext, Long Range Arena, etc. See in the <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">original Transformer paper</a> and references below.</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://arxiv.org/abs/2111.00396">Albert Gu, Karan Goel, Christopher Ré. "Efficiently Modeling Long Sequences with Structured State Spaces." ICLR 2022</a>  </li>
<li><a href="https://srush.github.io/annotated-s4/">Sasha Rush. "The Annotated S4."</a>  </li>
<li><a href="https://arxiv.org/pdf/2009.06732.pdf">Tay, Yi, Mostafa Dehghani, Dara Bahri, and Donald Metzler. "Efficient Transformers: A Survey." ArXiv 2020</a> (e.g. Transformer-XL, Reformer, Linformer, Linear transformer, Compressive Transformer, etc.)  </li>
<li><a href="https://arxiv.org/pdf/1909.00015.pdf">Gonçalo M. Correia, Vlad Niculae, André F.T. Martins. Adaptively Sparse Transformers. EMNLP 2019.</a>  </li>
<li><a href="https://arxiv.org/abs/2009.14794">Choromanski et al. "Rethinking Attention with Performers". ICLR 2021</a>  </li>
</ol>
</li>
</ul>
<hr />
<h1>Constrained Structured Classification with AD3</h1>
<ul>
<li><strong>Problem:</strong> Use AD3 and dual decomposition techniques to impose logic/budget/structured constraints in structured problems. Possible tasks could involve generating diverse output, forbidding certain configurations, etc.</li>
<li><strong>Data:</strong>  <ul>
<li><a href="http://rgai.inf.u-szeged.hu/conll2010st/download.html">"weasel words": detecting hedges/uncertainty in writing in order to improve clarity</a>  </li>
<li><a href="https://www.cs.umd.edu/~aguha/qbcoreference">Coreference in quizbowl</a></li>
</ul>
</li>
<li><strong>Evaluation:</strong> task-dependent</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="http://jmlr.org/papers/volume16/martins15a/martins15a.pdf">Martins, Figueiredo, Aguiar, Smith, Xing. AD3: Alternating Directions Dual Decomposition for MAP Inference in Graphical Models. JMLR 2015.</a>  </li>
<li><a href="http://aclweb.org/anthology/P17-1091">Niculae, Park, Cardie. Argument Mining with Structured SVMs and RNNs. ACL 2017.</a></li>
<li><a href="https://github.com/andre-martins/AD3">AD3 toolkit.</a>  </li>
</ol>
</li>
</ul>
<hr />
<h1>Hierarchical sparsemax attention</h1>
<ul>
<li><strong>Problem:</strong> Performing neural attention over very long sequences (e.g. for document-level classification, translation, ...)</li>
<li><strong>Method:</strong> sparse hierarchical attention with product of sparsemaxes.</li>
<li><strong>Data:</strong> <a href="https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M">text classification datasets</a></li>
<li><strong>Evaluation:</strong> Accuracy; empirical analysis of where the models attend to.</li>
<li><strong>Notes:</strong> If the top-level sparsemax gives zero probability to some paragraphs, those can be pruned from the computation graph. Can this lead to speedups?</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="http://www.aclweb.org/anthology/N16-1174">Yang, Yang, Dyer, He, Smola, Hovy. Hierarchical Attention Networks for Document Classification. NAACL 2016.</a>  </li>
<li><a href="http://proceedings.mlr.press/v48/martins16.html">Martins and Astudillo. From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification. ICML 2016.</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Energy-Based Models</h1>
<ul>
<li><strong>Problem:</strong> Energy networks can be used for density estimation (estimating p(x)) and structured prediction (estimating p(y|x)) when y is structured. Both cases pose challenges due to intractability of computing the partition function and sampling. In structured output prediction with energy networks, the idea is to replace discrete structured inference with continuous optimization in a neural net. This project can be either a survey about recent work in this area or it can explore some practical applications. Applications: multi-label classification and sequence tagging. </li>
<li><strong>Method:</strong> Learn a neural network E(x; w) to model the energy of x or E(x, y; w) to model the energy of an output configuration y (relaxed to be a continuous variable). Inference becomes min_y E(x, y; w). How far can this relaxation take us? Can it be better/faster than global combinatorial optimization approaches?</li>
<li><strong>Data:</strong> MNIST, multi-label classification, sequence tagging.</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="http://yann.lecun.com/exdb/publis/orig/lecun-06.pdf">LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., &amp; Huang, F. (2006). A tutorial on energy-based learning. Predicting structured data, 1(0).</a>.</li>
<li><a href="https://arxiv.org/abs/2103.03230">Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, Stéphane Deny. "Barlow Twins: Self-Supervised Learning via Redundancy Reduction." ICML 2021</a>.</li>
<li><a href="https://openreview.net/pdf?id=Hkxzx0NtDB">Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, Kevin Swersky (2020). Your classifier is secretly an energy based model and you should treat it like one. ICLR 2020.</a>. </li>
<li><a href="http://proceedings.mlr.press/v48/belanger16.pdf">Belanger and McCallum. Structured Prediction Energy Networks. ICML 2016.</a>  </li>
<li><a href="https://arxiv.org/abs/2101.03288">Yang Song, Diederik P. Kingma. "How to Train Your Energy-Based Models."</a>  </li>
</ol>
</li>
</ul>
<hr />
<h1>Memory-augmented Neural Networks</h1>
<ul>
<li><strong>Problem:</strong> Improve the generalization of neural nets by searching similar examples in the training set.</li>
<li><strong>Method:</strong> kNN + NN, fast search + NN, prototype attention (efficient attention over the dataset)</li>
<li><strong>Data:</strong> See in references below. </li>
<li><strong>References:</strong><ol>
<li><a href="https://openreview.net/pdf?id=HklBjCEKvH">Khandelwal, Urvashi, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through Memorization: Nearest Neighbor Language Models. ICLR 2020</a></li>
<li><a href="https://www.aclweb.org/anthology/P19-1533.pdf">Wiseman, Sam, and Karl Stratos. Label-Agnostic Sequence Labeling by Copying Nearest Neighbors. ACL 2019</a></li>
<li><a href="http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf">Lample, Guillaume, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. Large Memory Layers with Product Keys. NeurIPS 2019</a></li>
<li><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0,5&amp;q=A+Retrieve-and-Edit+Framework+for+Predicting+Structured+Outputs&amp;btnG=">Hashimoto, Tatsunori B., Kelvin Guu, Yonatan Oren, and Percy S. Liang. A retrieve-and-edit framework for predicting structured outputs. NeurIPS 2018</a></li>
<li><a href="https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00030">Guu, Kelvin, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang.  Generating Sentences by Editing Prototypes. TACL 2018</a></li>
<li><a href="https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00029">Tu, Zhaopeng, Yang Liu, Shuming Shi, and Tong Zhang. Learning to Remember Translation History with a Continuous Cache. TACL 2018</a></li>
<li><a href="https://www.aclweb.org/anthology/W18-5713.pdf">Weston, Jason, Emily Dinan, and Alexander H. Miller. Retrieve and Refine- Improved Sequence Generation Models For Dialogue. EMNLP - WSCAI 2018 </a></li>
<li><a href="https://www.aaai.org/GuideBook2018/17282-74380-GB.pdf">Gu, Jiatao, Yong Wang, Kyunghyun Cho, and Victor OK Li. Search Engine Guided Neural Machine Translation. AAAI 2018</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Quality Estimation and Uncertainty Estimation</h1>
<ul>
<li><strong>Problem:</strong> Estimate the quality of a translation hypothesis without access to reference translations.</li>
<li><strong>Method:</strong> See <a href="https://github.com/Unbabel/OpenKiwi">OpenKiwi</a>: Neural Nets, Transfer Learning, BERT, XLM, etc.</li>
<li><strong>Data:</strong> in <a href="http://www.statmt.org/wmt20/">WMT2020</a> page</li>
<li><strong>References:</strong> <ol>
<li><a href="https://www.aclweb.org/anthology/W15-3037.pdf">Kreutzer, Julia, Shigehiko Schamoni, and Stefan Riezler. QUality Estimation from ScraTCH (QUETCH): Deep Learning for Word-level Translation Quality Estimation. WMT 2015</a></li>
<li><a href="https://www.aclweb.org/anthology/W17-4763.pdf">Kim, Hyun, Jong-Hyeok Lee, and Seung-Hoon Na. Predictor-Estimator using Multilevel Task Learning with Stack Propagation for Neural Quality Estimation. WMT 2017</a></li>
<li><a href="http://statmt.org/wmt18/pdf/WMT093.pdf">Wang, Jiayi, Kai Fan, Bo Li, Fengming Zhou, Boxing Chen, Yangbin Shi, and Luo Si. Alibaba Submission for WMT18 Quality Estimation Task. WMT 2018</a>. </li>
<li><a href="https://www.aclweb.org/anthology/P19-3020.pdf">Fabio Kepler, Jonay Trénous, Marcos Treviso, Miguel Vera, André F. T. Martins. OpenKiwi: An Open Source Framework for Quality Estimation. ACL 2019.</a>. </li>
<li><a href="https://arxiv.org/pdf/2005.10608.pdf">Fomicheva, Marina, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia.  Unsupervised Quality Estimation for Neural Machine Translation. arXiv preprint 2020</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Causality and Disentanglement</h1>
<ul>
<li><strong>Problem:</strong> Causal inference and discovery is a area of growing interest in machine learning and statistics, with numerous applications and connections to confounding removal, reinforcement learning, and disentanglement of factors of variation. This project can be either a survey about the area or it can explore some practical applications.   </li>
<li><strong>Method:</strong> Plenty to choose from!</li>
<li><strong>Data:</strong> See the references below.  </li>
<li><strong>References:</strong> <ol>
<li><a href="https://icml.cc/Conferences/2020/ScheduleMultitrack?event=5752">Elias Bareimboim. Causal Reinforcement Learning. Tutorial in ICML 2020.</a>. </li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.474.pdf">Katherine A. Keith, David Jensen, and Brendan O'Connor. Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates.</a>. </li>
<li><a href="https://openreview.net/forum?id=ryxWIgBFPS">Bengio, Yoshua, Tristan Deleu, Nasim Rahaman, Rosemary Ke, Sébastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Pal. A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms. ICLR 2020 </a></li>
<li><a href="https://openreview.net/forum?id=S1g2skStPB">Zhu, Shengyu, Ignavier Ng, and Zhitang Chen. Causal Discovery with Reinforcement Learning. ICLR 2020</a></li>
<li><a href="https://arxiv.org/pdf/1911.10500.pdf">Schölkopf, Bernhard. Causality for Machine Learning</a></li>
<li><a href="https://www.aclweb.org/anthology/D17-1042.pdf">Alvarez-Melis, David, and Tommi S. Jaakkola. A causal framework for explaining the predictions of black-box sequence-to-sequence models. EMNLP 2017</a></li>
<li><a href="https://arxiv.org/abs/2203.02336">Gonçalo R. A. Faria, André F. T. Martins, Mário A. T. Figueiredo. Differentiable Causal Discovery Under Latent Interventions. CLEAR 2022.</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Emergent Communication</h1>
<ul>
<li><strong>Problem:</strong> Agents need to communicate to solve problems that require collaboration. The goal of this project is to apply techniques (for example using sparsemax or reinforcement learning) to induce communication among agents. </li>
<li><strong>Data:</strong> See references below.</li>
<li><strong>Evaluation:</strong> See references below.</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://papers.nips.cc/paper/6810-emergence-of-language-with-multi-agent-games-learning-to-communicate-with-sequences-of-symbols.pdf">Serhii Havrylov and Ivan Titov. Emergence of language with multi-agent games: Learning to communicate with sequences of symbols. NeurIPS 2017.</a></li>
<li><a href="https://openreview.net/pdf?id=Hk6WhagRW">Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, Stephen Clark. Emergent Communication through Negotiation. In ICLR 2018.</a></li>
<li><a href="https://www.aclweb.org/anthology/D17-1321.pdf">Satwik Kottur, José Moura, Stefan Lee, Dhruv Batra. Natural Language Does Not Emerge Naturally in Multi-Agent Dialog. In EMNLP 2017.</a></li>
</ol>
</li>
</ul>
<hr />
</section>
        <section id="extras" class="body">
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a rel="nofollow" href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a rel="nofollow" href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a rel="nofollow" href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>