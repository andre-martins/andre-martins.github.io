<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="generator" content="Pelican" />
        <title>Project Examples for Deep Structured Learning (Fall 2020)</title>
        <link rel="stylesheet" href="../theme/css/main.css" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="../">André F. T. Martins</a></h1>
                <nav><ul>
                    <li><a href="/index.html">Home</a></li>
                    <li><a href="/pages/jobs.html">Jobs</a></li>
                    <li><a href="/pages/publications.html">Publications</a></li>
                    <li><a href="/pages/software.html">Software</a></li>
                    <li><a href="/pages/courses.html">Courses</a></li>
                    <li><a href="/pages/sardine-lab.html">SARDINE Lab</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
    <h1 class="entry-title">Project Examples for Deep Structured Learning (Fall 2020)</h1>
    
    <p>We suggest below some project ideas. Feel free to use this as inspiration for your project. Talk to us for more details.</p>
<hr />
<h1>Emergent Communication</h1>
<ul>
<li><strong>Problem:</strong> Agents need to communicate to solve problems that require collaboration. The goal of this project is to apply techniques (for example using sparsemax or reinforcement learning) to induce communication among agents. </li>
<li><strong>Data:</strong> See references below.</li>
<li><strong>Evaluation:</strong> See references below.</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://papers.nips.cc/paper/6810-emergence-of-language-with-multi-agent-games-learning-to-communicate-with-sequences-of-symbols.pdf">Serhii Havrylov and Ivan Titov. Emergence of language with multi-agent games: Learning to communicate with sequences of symbols. NeurIPS 2017.</a></li>
<li><a href="https://openreview.net/pdf?id=Hk6WhagRW">Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, Stephen Clark. Emergent Communication through Negotiation. In ICLR 2018.</a></li>
<li><a href="https://www.aclweb.org/anthology/D17-1321.pdf">Satwik Kottur, José Moura, Stefan Lee, Dhruv Batra. Natural Language Does Not Emerge Naturally in Multi-Agent Dialog. In EMNLP 2017.</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Explainability of Neural Networks</h1>
<ul>
<li><strong>Problem:</strong> Neural networks are black boxes and not amenable to interpretation. The goal of this project is to develop and study methods that lead to explainability of neural network model's predictons (for example using sparsemax attention). This project can be either a survey about recent work in this area or it can explore some practical applications. </li>
<li><strong>Method:</strong> For example, sparse attention, rationalizers, gradient-based measures of feature importance, LIME, influence functions, etc.</li>
<li><strong>Data:</strong> BEER dataset, Stanford Sentiment Treebank, IMDB Large Movie Reviews Corpus, etc. See references below.</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf">Marco T Ribeiro, Sammer Singh, and Carlos Guestrin. Why Should I Trust You? Explaining the Predictions of Any Classifier. KDD 2016.</a>. </li>
<li><a href="https://people.csail.mit.edu/taolei/papers/emnlp16_rationale.pdf">Tao Lei, Regina Barzilay and Tommi Jaakkola. Rationalizing Neural Predictions. EMNLP 2016.</a>. </li>
<li><a href="https://arxiv.org/abs/2004.13876">Marcos V. Treviso, André F. T. Martins. Towards Prediction Explainability through Sparse Communication. Blackbox Workshop 2020.</a>. </li>
<li><a href="https://arxiv.org/pdf/1606.03490.pdf">Zachary  C  Lipton. The  mythos  of  model  interpretability. ICML 2016 Workshop on Human Interpretability in Machine Learning.</a>  </li>
<li><a href="https://arxiv.org/pdf/1908.04626.pdf">Sarah  Wiegreffe  and  Yuval  Pinter. Attention  is  not  not  explanation. EMNLP 2019.</a>. </li>
</ol>
</li>
</ul>
<hr />
<h1>Generative Adversarial Networks for Discrete Data</h1>
<ul>
<li><strong>Problem:</strong> Compare different deep generative models' ability to generate discrete data (such as text).</li>
<li><strong>Methods:</strong> Generative Adversarial Networks.</li>
<li><strong>Data:</strong> <a href="https://nlp.stanford.edu/projects/snli/">SNLI</a> (just the text), <a href="https://www.yelp.com/dataset/challenge">Yelp/Yahoo datasets for unaligned sentiment/topic transfer</a>, other text data.</li>
<li><strong>Evaluation:</strong> Some of the metrics in [3].</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://arxiv.org/abs/1406.2661">Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, Bengio. Generative Adversarial Networks. NIPS 2014.</a>  </li>
<li><a href="http://proceedings.mlr.press/v80/zhao18b/zhao18b.pdf">Zhao, Kim, Zhang, Rush, LeCun. Adversarially Regularized Autoencoders. ICML 2018.</a>  </li>
<li><a href="https://arxiv.org/abs/1806.04936">Semeniuta, Severyn, Gelly. On Accurate Evaluation of GANs for Language Generation. 2018.</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Sub-quadratic Transformers</h1>
<ul>
<li><strong>Problem:</strong> Transformers and BERT models are extremely large and expensive to train and keep in memory. The goal of this project is to make Transformers more efficient in terms of time and memory complexity by reducing the quadratic cost of self-attention or by inducing a sparser and smaller model.</li>
<li><strong>Method:</strong> See references below.</li>
<li><strong>Data:</strong> WMT datasets, WikiText, etc. See in the <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">original Transformer paper</a> and references below.</li>
<li><strong>References:</strong> <ol>
<li><a href="https://arxiv.org/pdf/2009.06732.pdf">Tay, Yi, Mostafa Dehghani, Dara Bahri, and Donald Metzler. "Efficient Transformers: A Survey." ArXiv 2020</a> (e.g. Transformer-XL, Reformer, Linformer, Linear transformer, Compressive Transformer, etc.)</li>
<li><a href="https://arxiv.org/pdf/1909.00015.pdf">Gonçalo M. Correia, Vlad Niculae, André F.T. Martins. Adaptively Sparse Transformers. EMNLP 2019.</a></li>
<li><a href="https://www.aclweb.org/anthology/P19-1032.pdf">Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, Armand Joulin. Adaptive Attention Span in Transformers. ACL 2019.</a></li>
<li><a href="https://arxiv.org/pdf/1904.10509.pdf">Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever. Generating Long Sequences with Sparse Transformers. Arxiv 2019.</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Contextual Probabilistic Embeddings / Language Modeling</h1>
<ul>
<li><strong>Problem:</strong> Embedding words as vectors (aka point masses) cannot distinguish between more vague or more specific concepts. One solution is to embed words as a mean vector μ and a covariance Σ. Muzellec &amp; Cuturi have a nice framework for this, tested for learning non-contextualized embeddings. Can we extend it to contextualized embeddings via language modelling? E.g. a model that reads an entire sentence and predicts a context-dependent pair (μ, Σ) for each word (perhaps left-to-right or masked). What likelihood to use? How can we evaluate the learned embeddings downstream?</li>
<li><strong>Method:</strong> See reference below.</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://arxiv.org/abs/1805.07594">Boris Muzellec, Marco Cuturi. Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions. Arxiv 2018.</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Constrained Structured Classification with AD3</h1>
<ul>
<li><strong>Problem:</strong> Use AD3 and dual decomposition techniques to impose logic/budget/structured constraints in structured problems. Possible tasks could involve generating diverse output, forbidding certain configurations, etc.</li>
<li><strong>Data:</strong>  <ul>
<li><a href="http://rgai.inf.u-szeged.hu/conll2010st/download.html">"weasel words": detecting hedges/uncertainty in writing in order to improve clarity</a>  </li>
<li><a href="https://www.cs.umd.edu/~aguha/qbcoreference">Coreference in quizbowl</a></li>
</ul>
</li>
<li><strong>Evaluation:</strong> task-dependent</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="http://jmlr.org/papers/volume16/martins15a/martins15a.pdf">Martins, Figueiredo, Aguiar, Smith, Xing. AD3: Alternating Directions Dual Decomposition for MAP Inference in Graphical Models. JMLR 2015.</a>  </li>
<li><a href="http://aclweb.org/anthology/P17-1091">Niculae, Park, Cardie. Argument Mining with Structured SVMs and RNNs. ACL 2017.</a></li>
<li><a href="https://github.com/andre-martins/AD3">AD3 toolkit.</a>  </li>
</ol>
</li>
</ul>
<hr />
<h1>Structured multi-label classification</h1>
<ul>
<li><strong>Problem:</strong> Multi-label classification is a learning setting where every sample can be assigned zero, one or more labels.</li>
<li><strong>Method:</strong> Correlations between labels can be exploited by learning an affinity matrix of label correlation. Inference in a fully-connected correlation graph is hard; approximating the graph by a tree makes inference fast (Viterbi can be used.)</li>
<li><strong>Data:</strong>  <a href="http://mulan.sourceforge.net/datasets-mlc.html">Multi-label datasets</a></li>
<li><strong>Evaluation:</strong> see <a href="http://scikit-learn.org/stable/modules/model_evaluation.html#multilabel-ranking-metrics">here</a></li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://pdfs.semanticscholar.org/6b56/91db1e3a79af5e3c136d2dd322016a687a0b.pdf">Sorower. A Literature Survey on Algorithms for Multi-label Learning.</a>  </li>
<li><a href="https://www.cs.cornell.edu/people/tj/publications/finley_joachims_08a.pdf">Thomas Finley and Thorsten Joachims. 2008. Training structural SVMs when exact inference is intractable.</a>  </li>
<li><a href="https://pystruct.github.io/user_guide.html#multi-label-svm">Pystruct</a>  </li>
<li><a href="http://scikit.ml/">Scikit.ml</a> (very strong methods not based on structured prediction)</li>
</ol>
</li>
</ul>
<hr />
<h1>Hierarchical sparsemax attention</h1>
<ul>
<li><strong>Problem:</strong> Performing neural attention over very long sequences (e.g. for document-level classification, translation, ...)</li>
<li><strong>Method:</strong> sparse hierarchical attention with product of sparsemaxes.</li>
<li><strong>Data:</strong> <a href="https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M">text classification datasets</a></li>
<li><strong>Evaluation:</strong> Accuracy; empirical analysis of where the models attend to.</li>
<li><strong>Notes:</strong> If the top-level sparsemax gives zero probability to some paragraphs, those can be pruned from the computation graph. Can this lead to speedups?</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="http://www.aclweb.org/anthology/N16-1174">Yang, Yang, Dyer, He, Smola, Hovy. Hierarchical Attention Networks for Document Classification. NAACL 2016.</a>  </li>
<li><a href="http://proceedings.mlr.press/v48/martins16.html">Martins and Astudillo. From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification. ICML 2016.</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Sparse link prediction</h1>
<ul>
<li><strong>Problem:</strong> Predicting links in a large structured graph. For instance: predict co-authorship, movie recommendation, coreference resolution, discourse relations between sentences in a document.</li>
<li><strong>Method:</strong> The simplest approach is independent binary classification: for every node pair (i, j), predict whether there is a link or not. Issues: Very high imbalance: most nodes are not linked.
Structure and higher-order correlations are ignored in independent approach. Develop a method that can address the issues: incorporate structural correlations (e.g. with combinatorial inference, constraints, latent variables) and account for imbalance (ideally via pairwise ranking losses: learn a scorer such that S(i, j) &gt; S(k, l) if there is an edge (i, j) but no edge (k, l).</li>
<li><strong>Data:</strong> <a href="https://github.com/CornellNLP/Macros">arXiv macro usage</a>, <a href="https://www.cs.umd.edu/~aguha/qbcoreference">Coreference in quizbowl</a></li>
<li><strong>Notes:</strong> Can graph-CNNs (previous idea) be useful here?</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="http://ttic.uchicago.edu/~meshi/papers/structAUC_aistats14.pdf">Rosenfeld, Meshi, Tarlow, Globerson. Learning Structured Models with the AUC Loss and Its Generalizations. AISTATS 2014.</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Energy Networks</h1>
<ul>
<li><strong>Problem:</strong> Energy networks can be used for density estimation (estimating p(x)) and structured prediction (estimating p(y|x)) when y is structured. Both cases pose challenges due to intractability of computing the partition function and sampling. In structured output prediction with energy networks, the idea is to replace discrete structured inference with continuous optimization in a neural net. This project can be either a survey about recent work in this area or it can explore some practical applications. Applications: multi-label classification and sequence tagging. </li>
<li><strong>Method:</strong> Learn a neural network E(x; w) to model the energy of x or E(x, y; w) to model the energy of an output configuration y (relaxed to be a continuous variable). Inference becomes min_y E(x, y; w). How far can this relaxation take us? Can it be better/faster than global combinatorial optimization approaches?</li>
<li><strong>Data:</strong> MNIST, multi-label classification, sequence tagging.</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="http://yann.lecun.com/exdb/publis/orig/lecun-06.pdf">LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., &amp; Huang, F. (2006). A tutorial on energy-based learning. Predicting structured data, 1(0).</a>.  </li>
<li><a href="https://openreview.net/pdf?id=Hkxzx0NtDB">Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, Kevin Swersky (2020). Your classifier is secretly an energy based model and you should treat it like one. ICLR 2020.</a>. </li>
<li><a href="http://proceedings.mlr.press/v48/belanger16.pdf">Belanger and McCallum. Structured Prediction Energy Networks. ICML 2016.</a>  </li>
<li><a href="https://arxiv.org/abs/1703.05667">Belager, Yang, McCallum. End-to-End Learning for Structured Prediction Energy Networks. ICML 2017.</a>  </li>
</ol>
</li>
</ul>
<hr />
<h1>Memory-augmented Neural Networks</h1>
<ul>
<li><strong>Problem:</strong> Improve the generalization of neural nets by searching similar examples in the training set.</li>
<li><strong>Method:</strong> kNN + NN, fast search + NN, prototype attention (efficient attention over the dataset)</li>
<li><strong>Data:</strong> See in references below. </li>
<li><strong>References:</strong><ol>
<li><a href="https://openreview.net/pdf?id=HklBjCEKvH">Khandelwal, Urvashi, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through Memorization: Nearest Neighbor Language Models. ICLR 2020</a></li>
<li><a href="https://www.aclweb.org/anthology/P19-1533.pdf">Wiseman, Sam, and Karl Stratos. Label-Agnostic Sequence Labeling by Copying Nearest Neighbors. ACL 2019</a></li>
<li><a href="http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf">Lample, Guillaume, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. Large Memory Layers with Product Keys. NeurIPS 2019</a></li>
<li><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0,5&amp;q=A+Retrieve-and-Edit+Framework+for+Predicting+Structured+Outputs&amp;btnG=">Hashimoto, Tatsunori B., Kelvin Guu, Yonatan Oren, and Percy S. Liang. A retrieve-and-edit framework for predicting structured outputs. NeurIPS 2018</a></li>
<li><a href="https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00030">Guu, Kelvin, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang.  Generating Sentences by Editing Prototypes. TACL 2018</a></li>
<li><a href="https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00029">Tu, Zhaopeng, Yang Liu, Shuming Shi, and Tong Zhang. Learning to Remember Translation History with a Continuous Cache. TACL 2018</a></li>
<li><a href="https://www.aclweb.org/anthology/W18-5713.pdf">Weston, Jason, Emily Dinan, and Alexander H. Miller. Retrieve and Refine- Improved Sequence Generation Models For Dialogue. EMNLP - WSCAI 2018 </a></li>
<li><a href="https://www.aaai.org/GuideBook2018/17282-74380-GB.pdf">Gu, Jiatao, Yong Wang, Kyunghyun Cho, and Victor OK Li. Search Engine Guided Neural Machine Translation. AAAI 2018</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Quality Estimation and Uncertainty Estimation</h1>
<ul>
<li><strong>Problem:</strong> Estimate the quality of a translation hypothesis without access to reference translations.</li>
<li><strong>Method:</strong> See <a href="https://github.com/Unbabel/OpenKiwi">OpenKiwi</a>: Neural Nets, Transfer Learning, BERT, XLM, etc.</li>
<li><strong>Data:</strong> in <a href="http://www.statmt.org/wmt20/">WMT2020</a> page</li>
<li><strong>References:</strong> <ol>
<li><a href="https://www.aclweb.org/anthology/W15-3037.pdf">Kreutzer, Julia, Shigehiko Schamoni, and Stefan Riezler. QUality Estimation from ScraTCH (QUETCH): Deep Learning for Word-level Translation Quality Estimation. WMT 2015</a></li>
<li><a href="https://www.aclweb.org/anthology/W17-4763.pdf">Kim, Hyun, Jong-Hyeok Lee, and Seung-Hoon Na. Predictor-Estimator using Multilevel Task Learning with Stack Propagation for Neural Quality Estimation. WMT 2017</a></li>
<li><a href="http://statmt.org/wmt18/pdf/WMT093.pdf">Wang, Jiayi, Kai Fan, Bo Li, Fengming Zhou, Boxing Chen, Yangbin Shi, and Luo Si. Alibaba Submission for WMT18 Quality Estimation Task. WMT 2018</a>. </li>
<li><a href="https://www.aclweb.org/anthology/P19-3020.pdf">Fabio Kepler, Jonay Trénous, Marcos Treviso, Miguel Vera, André F. T. Martins. OpenKiwi: An Open Source Framework for Quality Estimation. ACL 2019.</a>. </li>
<li><a href="https://arxiv.org/pdf/2005.10608.pdf">Fomicheva, Marina, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia.  Unsupervised Quality Estimation for Neural Machine Translation. arXiv preprint 2020</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Causality and Disentanglement</h1>
<ul>
<li><strong>Problem:</strong> Causal inference and discovery is a area of growing interest in machine learning and statistics, with numerous applications and connections to confounding removal, reinforcement learning, and disentanglement of factors of variation. This project can be either a survey about the area or it can explore some practical applications.   </li>
<li><strong>Method:</strong> Plenty to choose from!</li>
<li><strong>Data:</strong> See the references below.  </li>
<li><strong>References:</strong> <ol>
<li><a href="https://icml.cc/Conferences/2020/ScheduleMultitrack?event=5752">Elias Bareimboim. Causal Reinforcement Learning. Tutorial in ICML 2020.</a>. </li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.474.pdf">Katherine A. Keith, David Jensen, and Brendan O'Connor. Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates.</a>. </li>
<li><a href="https://openreview.net/forum?id=ryxWIgBFPS">Bengio, Yoshua, Tristan Deleu, Nasim Rahaman, Rosemary Ke, Sébastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Pal. A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms. ICLR 2020 </a></li>
<li><a href="https://openreview.net/forum?id=S1g2skStPB">Zhu, Shengyu, Ignavier Ng, and Zhitang Chen. Causal Discovery with Reinforcement Learning. ICLR 2020</a></li>
<li><a href="https://arxiv.org/pdf/1911.10500.pdf">Schölkopf, Bernhard. Causality for Machine Learning</a></li>
<li><a href="https://www.aclweb.org/anthology/D17-1042.pdf">Alvarez-Melis, David, and Tommi S. Jaakkola. A causal framework for explaining the predictions of black-box sequence-to-sequence models. EMNLP 2017</a></li>
</ol>
</li>
</ul>
</section>
        <section id="extras" class="body">
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>