<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>Deep Structured Learning (IST, Fall 2018)</title>
        <link rel="stylesheet" href="../theme/css/main.css" />

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="../">André F. T. Martins </a></h1>
                <nav><ul>
                    <li><a href="/index.html">Home</a></li>
                    <li><a href="/pages/jobs.html">Jobs</a></li>
                    <li><a href="/pages/publications.html">Publications</a></li>
                    <li><a href="/pages/software.html">Software</a></li>
                    <li><a href="/pages/courses.html">Courses</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
    <h1 class="entry-title">Deep Structured Learning (IST, Fall 2018)</h1>
    
    <h1>Summary</h1>
<p><strong>Structured prediction</strong> is a framework in machine learning which deals with structured and highly interdependent output variables, with applications in natural language processing, computer vision, computational biology, and signal processing.
In the last 5 years, several applications in these areas achieved new breakthroughs by replacing the traditional feature-based linear models by more powerful deep learning models based on neural networks, capable of learning internal representations.</p>
<p>In this course, I will describe methods, models, and algorithms for structured prediction, ranging from "shallow" <strong>linear models</strong> (hidden Markov models, conditional random fields, structured support vector machines) to modern <strong>deep learning models</strong> (convolutional networks, recurrent neural networks, attention mechanisms, etc.), passing through shallow and deep methods akin to reinforcement learning. Representation learning will also be discussed (PCA, auto-encoders, and various deep generative models).
The theoretical concepts taught in this course will be complemented by a strong practical component, by letting students work in group projects where they can solve practical problems by using software suitable for deep learning (e.g., Pytorch, TensorFlow, DyNet).</p>
<hr />
<h1>Course Information</h1>
<ul>
<li><strong>Instructor:</strong> <a href="http://andre-martins.github.io">André Martins</a></li>
<li><strong>Schedule:</strong> Wednesdays 14:30-18:00 (tentative)</li>
<li><strong>Mailing list</strong>: TBD</li>
</ul>
<hr />
<h1>Grading</h1>
<ul>
<li>Homework assignments (60%)</li>
<li>Final project (40%)</li>
</ul>
<hr />
<h1>Project Examples</h1>
<p>TBD</p>
<hr />
<h1>Recommended Bibliography</h1>
<ul>
<li><a href="http://www.deeplearningbook.org">Deep Learning.</a> Ian Goodfellow and Yoshua Bengio and Aaron Courville. MIT Press, 2016.</li>
<li>Machine Learning: a Probabilistic Perspective. Kevin P. Murphy. MIT Press, 2013.</li>
<li>Linguistic Structured Prediction. Noah A. Smith. Morgan &amp; Claypool Synthesis Lectures on Human Language Technologies. 2011.</li>
</ul>
<hr />
<h1>Schedule</h1>
<table class="table table-condensed table-bordered table-hover">
<colgroup>
  <col span="1" style="width: 10%;">
  <col span="1" style="width: 45%;">
  <col span="1" style="width: 45%;">
</colgroup>

<tr>
<th>Date</th>
<th>Topic</th>
<th>Optional Reading</th>
</tr>

<tr>
<td><b>Sep 19</b></td>
<td>
Introduction and Course Description
</td>
<td>
<a href="http://lxmls.it.pt/2018/Figueiredo_LxMLS2018.pdf">Mário Figueiredo's LxMLS intro lecture</a><br/>
<a href="https://github.com/luispedro/talk-python-intro">Luis Pedro Coelho's intro to Python</a><br/>
Goodfellow et al. Ch. 1-5<br/>
Murphy Ch. 1-2
</td>
</tr>

<tr>
<td><b>Sep 26</b></td>
<td>Linear Classifiers</td>
<td>
Murphy Ch. 3, 6, 8-9, 14
</td>
</tr>

<tr>
<td><b>Oct 3</b></td>
<td>Feedforward Neural Networks</td>
<td>
Goodfellow et al. Ch. 6
</td>
</tr>

<tr>
<td><b>Oct 10</b></td>
<td>Training Neural Networks</td>
<td>
Goodfellow et al. Ch. 7-8
</td>
</tr>

<tr>
<td><b>Oct 17</b></td>
<td>Linear Sequence Models</td>
<td>
Smith, Ch. 3-4<br/>
Murphy Ch. 17, 19
</td>
</tr>

<tr>
<td><b>Oct 24</b></td>
<td>Representation Learning and Convolutional Neural Networks</td>
<td>
Goodfellow et al. Ch. 9, 14-15
</td>
</tr>

<tr>
<td><b>Oct 31</b></td>
<td>Structured Prediction and Graphical Models</td>
<td>
Murphy Ch. 10, 19-22
Goodfellow et al. Ch. 16<br/>
</td>
<td>
</td>
</tr>

<tr>
<td><b>Nov 7</b></td>
<td>Recurrent Neural Networks</td>
<td>
Goodfellow et al. Ch. 10
</td>
</tr>

<tr>
<td><b>Nov 14</b></td>
<td>Sequence-to-Sequence Learning</td>
<td>
</td>
</tr>

<tr>
<td><b>Nov 21</b></td>
<td>Attention Mechanisms and Neural Memories</td>
<td>
</td>
</tr>

<tr>
<td><b>Nov 28</b></td>
<td>Deep Reinforcement Learning</td>
<td>
</td>
</tr>

<tr>
<td><b>Dec 5</b></td>
<td>Deep Generative Models (Variational Auto-Encoders and Generative Adversarial Networks)</td>
<td>
Goodfellow et al. Ch. 20<br/>
Murphy, Ch. 28
</td>
</tr>

<tr>
<td><b>Dec 12</b></td>
<td>Final Projects I</td>
<td>
</td>
</tr>

<tr>
<td><b>Dec 19</b></td>
<td>Final Projects II</td>
<td>
</td>
</tr>


</table>
</section>
        <section id="extras" class="body">
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>