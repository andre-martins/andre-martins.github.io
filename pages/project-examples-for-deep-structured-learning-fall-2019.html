<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="generator" content="Pelican" />
        <title>Project Examples for Deep Structured Learning (Fall 2019)</title>
        <link rel="stylesheet" href="../theme/css/main.css" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="../">André F. T. Martins</a></h1>
                <nav><ul>
                    <li><a href="/index.html">Home</a></li>
                    <li><a href="/pages/jobs.html">Jobs</a></li>
                    <li><a href="/pages/publications.html">Publications</a></li>
                    <li><a href="/pages/software.html">Software</a></li>
                    <li><a href="/pages/courses.html">Courses</a></li>
                    <li><a href="/pages/sardine.html">SARDINE Lab</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
    <h1 class="entry-title">Project Examples for Deep Structured Learning (Fall 2019)</h1>
    
    <p>We suggest below some project ideas. Feel free to use this as inspiration for your project. Talk to us for more details.</p>
<hr />
<h1>Emergent Communication</h1>
<ul>
<li><strong>Problem:</strong> Agents need to communicate to solve problems that require collaboration. The goal of this project is to apply techniques (for example using sparsemax or reinforcement learning) to induce communication among agents. </li>
<li><strong>Data:</strong> See references below.</li>
<li><strong>Evaluation:</strong> See references below.</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://papers.nips.cc/paper/6810-emergence-of-language-with-multi-agent-games-learning-to-communicate-with-sequences-of-symbols.pdf">Serhii Havrylov and Ivan Titov. Emergence of language with multi-agent games: Learning to communicate with sequences of symbols. NeurIPS 2017.</a></li>
<li><a href="https://openreview.net/pdf?id=Hk6WhagRW">Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, Stephen Clark. Emergent Communication through Negotiation. In ICLR 2018.</a></li>
<li><a href="https://www.aclweb.org/anthology/D17-1321.pdf">Satwik Kottur, José Moura, Stefan Lee, Dhruv Batra. Natural Language Does Not Emerge Naturally in Multi-Agent Dialog. In EMNLP 2017.</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Explainability of Neural Networks</h1>
<ul>
<li><strong>Problem:</strong> Neural networks are black boxes and not amenable to interpretation. The goal of this project is to develop and study methods that lead to explainability of neural network model's predictons (for example using sparsemax attention).</li>
<li><strong>Method:</strong> For example, sparse attention, gradient-based measures of feature importance, LIME (see below).</li>
<li><strong>Data:</strong> Stanford Sentiment Treebank, IMDB Large Movie Reviews Corpus, etc. See references below.</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf">Marco T Ribeiro, Sammer Singh, and Carlos Guestrin. Why Should I Trust You? Explaining the Predictions of Any Classifier. KDD 2016.</a></li>
<li><a href="https://www.aclweb.org/anthology/N19-1357.pdf">Sarthak Jain and Byron C Wallace. Attention is not explanation. NAACL 2019.</a></li>
<li><a href="https://arxiv.org/pdf/1606.03490.pdf">Zachary  C  Lipton. The  mythos  of  model  interpretability. ICML 2016 Workshop on Human Interpretability in Machine Learning.</a></li>
<li><a href="https://www.aclweb.org/anthology/P19-1282.pdf">Sofia Serrano and Noah A Smith. Is attention interpretable? ACL 2019.</a></li>
<li><a href="https://arxiv.org/pdf/1908.04626.pdf">Sarah  Wiegreffe  and  Yuval  Pinter. Attention  is  not  not  explanation. EMNLP 2019.</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Generative Adversarial Networks for Discrete Data</h1>
<ul>
<li><strong>Problem:</strong> Compare different deep generative models' ability to generate discrete data (such as text).</li>
<li><strong>Methods:</strong> Generative Adversarial Networks.</li>
<li><strong>Data:</strong> <a href="https://nlp.stanford.edu/projects/snli/">SNLI</a> (just the text), <a href="https://www.yelp.com/dataset/challenge">Yelp/Yahoo datasets for unaligned sentiment/topic transfer</a>, other text data.</li>
<li><strong>Evaluation:</strong> Some of the metrics in [3].</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://arxiv.org/abs/1406.2661">Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, Bengio. Generative Adversarial Networks. NIPS 2014.</a>  </li>
<li><a href="http://proceedings.mlr.press/v80/zhao18b/zhao18b.pdf">Zhao, Kim, Zhang, Rush, LeCun. Adversarially Regularized Autoencoders. ICML 2018.</a>  </li>
<li><a href="https://arxiv.org/abs/1806.04936">Semeniuta, Severyn, Gelly. On Accurate Evaluation of GANs for Language Generation. 2018.</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Object detection with weak supervision</h1>
<ul>
<li><strong>Problem:</strong> Given images with object tags, can we accurately predict object location by training only with coarse, weak supervision about which objects are present in the data?</li>
<li><strong>Method:</strong> Latent Potts model; Belief Propagation</li>
<li><strong>Data:</strong> COCO-Stuff, Pascal VOC </li>
<li><strong>References:</strong> See https://github.com/kazuto1011/deeplab-pytorch</li>
</ul>
<hr />
<h1>Sparse transformers</h1>
<ul>
<li><strong>Problem:</strong> Transformers and BERT models are extremely large and expensive to train and keep in memory. The goal of this project is to distill or induce sparser and smaller Transformer models without losing accuracy, applying them to machine translation or language modeling.</li>
<li><strong>Method:</strong> See references below.</li>
<li><strong>Data:</strong> WMT datasets, WikiText, etc. See references below.</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://arxiv.org/pdf/1909.00015.pdf">Gonçalo M. Correia, Vlad Niculae, André F.T. Martins. Adaptively Sparse Transformers. EMNLP 2019.</a></li>
<li><a href="https://www.aclweb.org/anthology/P19-1032.pdf">Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, Armand Joulin. Adaptive Attention Span in Transformers. ACL 2019.</a></li>
<li><a href="https://arxiv.org/pdf/1904.10509.pdf">Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever. Generating Long Sequences with Sparse Transformers. Arxiv 2019.</a></li>
<li><a href="https://arxiv.org/pdf/1909.10351.pdf">Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu. TinyBERT: Distilling BERT for Natural Language Understanding. Arxiv 2019.</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Contextual Probabilistic Embeddings / Language Modeling</h1>
<ul>
<li><strong>Problem:</strong> Embedding words as vectors (aka point masses) cannot distinguish between more vague or more specific concepts. One solution is to embed words as a mean vector μ and a covariance Σ. Muzellec &amp; Cuturi have a nice framework for this, tested for learning non-contextualized embeddings. Can we extend it to contextualized embeddings via language modelling? E.g. a model that reads an entire sentence and predicts a context-dependent pair (μ, Σ) for each word (perhaps left-to-right or masked). What likelihood to use? How can we evaluate the learned embeddings downstream?</li>
<li><strong>Method:</strong> See reference below.</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://arxiv.org/abs/1805.07594">Boris Muzellec, Marco Cuturi. Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions. Arxiv 2018.</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Constrained Structured Classification with AD3</h1>
<ul>
<li><strong>Problem:</strong> Use AD3 and dual decomposition techniques to impose logic/budget/structured constraints in structured problems. Possible tasks could involve generating diverse output, forbidding certain configurations, etc.</li>
<li><strong>Data:</strong>  <ul>
<li><a href="http://rgai.inf.u-szeged.hu/conll2010st/download.html">"weasel words": detecting hedges/uncertainty in writing in order to improve clarity</a>  </li>
<li><a href="https://www.cs.umd.edu/~aguha/qbcoreference">Coreference in quizbowl</a></li>
</ul>
</li>
<li><strong>Evaluation:</strong> task-dependent</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="http://jmlr.org/papers/volume16/martins15a/martins15a.pdf">Martins, Figueiredo, Aguiar, Smith, Xing. AD3: Alternating Directions Dual Decomposition for MAP Inference in Graphical Models. JMLR 2015.</a>  </li>
<li><a href="http://aclweb.org/anthology/P17-1091">Niculae, Park, Cardie. Argument Mining with Structured SVMs and RNNs. ACL 2017.</a></li>
<li><a href="https://github.com/andre-martins/AD3">AD3 toolkit.</a>  </li>
</ol>
</li>
</ul>
<hr />
<h1>Structured multi-label classification</h1>
<ul>
<li><strong>Problem:</strong> Multi-label classification is a learning setting where every sample can be assigned zero, one or more labels.</li>
<li><strong>Method:</strong> Correlations between labels can be exploited by learning an affinity matrix of label correlation. Inference in a fully-connected correlation graph is hard; approximating the graph by a tree makes inference fast (Viterbi can be used.)</li>
<li><strong>Data:</strong>  <a href="http://mulan.sourceforge.net/datasets-mlc.html">Multi-label datasets</a></li>
<li><strong>Evaluation:</strong> see <a href="http://scikit-learn.org/stable/modules/model_evaluation.html#multilabel-ranking-metrics">here</a></li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://pdfs.semanticscholar.org/6b56/91db1e3a79af5e3c136d2dd322016a687a0b.pdf">Sorower. A Literature Survey on Algorithms for Multi-label Learning.</a>  </li>
<li><a href="https://www.cs.cornell.edu/people/tj/publications/finley_joachims_08a.pdf">Thomas Finley and Thorsten Joachims. 2008. Training structural SVMs when exact inference is intractable.</a>  </li>
<li><a href="https://pystruct.github.io/user_guide.html#multi-label-svm">Pystruct</a>  </li>
<li><a href="http://scikit.ml/">Scikit.ml</a> (very strong methods not based on structured prediction)</li>
</ol>
</li>
</ul>
<hr />
<h1>Hierarchical sparsemax attention</h1>
<ul>
<li><strong>Problem:</strong> Performing neural attention over very long sequences (e.g. for document-level classification, translation, ...)</li>
<li><strong>Method:</strong> sparse hierarchical attention with product of sparsemaxes.</li>
<li><strong>Data:</strong> <a href="https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M">text classification datasets</a></li>
<li><strong>Evaluation:</strong> Accuracy; empirical analysis of where the models attend to.</li>
<li><strong>Notes:</strong> If the top-level sparsemax gives zero probability to some paragraphs, those can be pruned from the computation graph. Can this lead to speedups?</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="http://www.aclweb.org/anthology/N16-1174">Yang, Yang, Dyer, He, Smola, Hovy. Hierarchical Attention Networks for Document Classification. NAACL 2016.</a>  </li>
<li><a href="http://proceedings.mlr.press/v48/martins16.html">Martins and Astudillo. From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification. ICML 2016.</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Sparse group lasso attention mechanism</h1>
<ul>
<li><strong>Problem:</strong> For structured data segmented into given "groups" (e.g. fields in a form, regions in an image, sentences in a paragraph), design a "group-sparse" attention mechanism that tends to give zero weight to entire groups when deemed not relevant enough.</li>
<li><strong>Method:</strong> a Sparse Group-Lasso penalty in a generalized structured attention framework [2]</li>
<li><strong>Notes:</strong> the L1 term is redundant when optimizing over the simplex; regular group lasso will be sparse!</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="https://arxiv.org/abs/1001.0736">A note on the group lasso and a sparse group lasso. J. Friedman, T. Hastie, R. Tibshirani. 2010.</a>  </li>
<li><a href="https://papers.nips.cc/paper/6926-a-regularized-framework-for-sparse-and-structured-neural-attention.pdf">A Regularized Framework for Sparse and Structured Neural Attention. Vlad Niculae, Mathieu Blondel. NIPS 2017.</a>  </li>
<li><a href="https://www.stat.wisc.edu/~myuan/papers/glasso.final.pdf">Model selection and estimation in regression with grouped variables. Yuan and Lin. 2006.</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Sparse link prediction</h1>
<ul>
<li><strong>Problem:</strong> Predicting links in a large structured graph. For instance: predict co-authorship, movie recommendation, coreference resolution, discourse relations between sentences in a document.</li>
<li><strong>Method:</strong> The simplest approach is independent binary classification: for every node pair (i, j), predict whether there is a link or not. Issues: Very high imbalance: most nodes are not linked.
Structure and higher-order correlations are ignored in independent approach. Develop a method that can address the issues: incorporate structural correlations (e.g. with combinatorial inference, constraints, latent variables) and account for imbalance (ideally via pairwise ranking losses: learn a scorer such that S(i, j) &gt; S(k, l) if there is an edge (i, j) but no edge (k, l).</li>
<li><strong>Data:</strong> <a href="https://github.com/CornellNLP/Macros">arXiv macro usage</a>, <a href="https://www.cs.umd.edu/~aguha/qbcoreference">Coreference in quizbowl</a></li>
<li><strong>Notes:</strong> Can graph-CNNs (previous idea) be useful here?</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="http://ttic.uchicago.edu/~meshi/papers/structAUC_aistats14.pdf">Rosenfeld, Meshi, Tarlow, Globerson. Learning Structured Models with the AUC Loss and Its Generalizations. AISTATS 2014.</a></li>
</ol>
</li>
</ul>
<hr />
<h1>Structured Prediction Energy Networks</h1>
<ul>
<li><strong>Problem:</strong> Structured output prediction with energy networks: replace discrete structured inference with continuous optimization in a neural net. Applications: multi-label classification; simple structured problems: sequence tagging, arc-factored parsing?</li>
<li><strong>Method:</strong> Learn a neural network E(x, y; w) to model the energy of an output configuration y (relaxed to be a continuous variable). Inference becomes min_y E(x, y; w). How far can this relaxation take us? Can it be better/faster than global combinatorial optimization approaches?</li>
<li><strong>Data:</strong> Sequence tagging, parsing, optimal matching?</li>
<li><strong>Notes:</strong> When E is a neural network, min_y E(x, y; w) is a non-convex optimization problem (possibly with mild constraints such as y in [0, 1]. Amos et al. have an approach that allows E to be a complicated neural net but remain convex in y. Is this beneficial? Are some kinds of structured data better suited for SPENs than others? E.g. sequence labelling seems "less structured" than dependency parsing.</li>
<li>
<p><strong>References:</strong></p>
<ol>
<li><a href="http://proceedings.mlr.press/v48/belanger16.pdf">Belanger and McCallum. Structured Prediction Energy Networks. ICML 2016.</a>  </li>
<li><a href="https://arxiv.org/abs/1703.05667">Belager, Yang, McCallum. End-to-End Learning for Structured Prediction Energy Networks. ICML 2017.</a>  </li>
<li><a href="http://proceedings.mlr.press/v70/amos17b/amos17b.pdf">Amos, Xu, Kolter. Input Convex Neural Networks. ICML 2017</a></li>
</ol>
</li>
</ul>
</section>
        <section id="extras" class="body">
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a rel="nofollow" href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a rel="nofollow" href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a rel="nofollow" href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>