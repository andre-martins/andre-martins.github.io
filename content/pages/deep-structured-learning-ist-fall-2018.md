Title: Deep Structured Learning (IST, Fall 2018)

# Summary

**Structured prediction** is a framework in machine learning which deals with structured and highly interdependent output variables, with applications in natural language processing, computer vision, computational biology, and signal processing.
In the last 5 years, several applications in these areas achieved new breakthroughs by replacing the traditional feature-based linear models by more powerful deep learning models based on neural networks, capable of learning internal representations.

In this course, I will describe methods, models, and algorithms for structured prediction, ranging from "shallow" **linear models** (hidden Markov models, conditional random fields, structured support vector machines) to modern **deep learning models** (convolutional networks, recurrent neural networks, attention mechanisms, etc.), passing through shallow and deep methods akin to reinforcement learning. Representation learning will also be discussed (PCA, auto-encoders, and various deep generative models).
The theoretical concepts taught in this course will be complemented by a strong practical component, by letting students work in group projects where they can solve practical problems by using software suitable for deep learning (e.g., Pytorch, TensorFlow, DyNet).

---

# Course Information

- **Instructor:** [André Martins](http://andre-martins.github.io)
- **Schedule:** Wednesdays 14:30-18:00 (tentative)
- **Mailing list**: TBD

---

# Grading

- Homework assignments (60%)
- Final project (40%)

---

# Project Examples

TBD

---

# Recommended Bibliography

- [Deep Learning.](http://www.deeplearningbook.org) Ian Goodfellow and Yoshua Bengio and Aaron Courville. MIT Press, 2016.
- Machine Learning: a Probabilistic Perspective. Kevin P. Murphy. MIT Press, 2013.
- Linguistic Structured Prediction. Noah A. Smith. Morgan & Claypool Synthesis Lectures on Human Language Technologies. 2011.

---

# Schedule

<table class="table table-condensed table-bordered table-hover">
<colgroup>
  <col span="1" style="width: 10%;">
  <col span="1" style="width: 45%;">
  <col span="1" style="width: 45%;">
</colgroup>

<tr>
<th>Date</th>
<th>Topic</th>
<th>Optional Reading</th>
</tr>

<tr>
<td><b>Sep 19</b></td>
<td>
Introduction and Course Description
</td>
<td>
<a href="http://lxmls.it.pt/2018/Figueiredo_LxMLS2018.pdf">Mário Figueiredo's LxMLS intro lecture</a><br/>
<a href="https://github.com/luispedro/talk-python-intro">Luis Pedro Coelho's intro to Python</a><br/>
Goodfellow et al. Ch. 1-5<br/>
Murphy Ch. 1-2
</td>
</tr>

<tr>
<td><b>Sep 26</b></td>
<td>Linear Classifiers</td>
<td>
Murphy Ch. 3, 6, 8-9, 14
</td>
</tr>

<tr>
<td><b>Oct 3</b></td>
<td>Feedforward Neural Networks</td>
<td>
Goodfellow et al. Ch. 6
</td>
</tr>

<tr>
<td><b>Oct 10</b></td>
<td>Training Neural Networks</td>
<td>
Goodfellow et al. Ch. 7-8
</td>
</tr>

<tr>
<td><b>Oct 17</b></td>
<td>Linear Sequence Models</td>
<td>
Smith, Ch. 3-4<br/>
Murphy Ch. 17, 19
</td>
</tr>

<tr>
<td><b>Oct 24</b></td>
<td>Representation Learning and Convolutional Neural Networks</td>
<td>
Goodfellow et al. Ch. 9, 14-15
</td>
</tr>

<tr>
<td><b>Oct 31</b></td>
<td>Structured Prediction and Graphical Models</td>
<td>
Murphy Ch. 10, 19-22
Goodfellow et al. Ch. 16<br/>
</td>
<td>
</td>
</tr>

<tr>
<td><b>Nov 7</b></td>
<td>Recurrent Neural Networks</td>
<td>
Goodfellow et al. Ch. 10
</td>
</tr>

<tr>
<td><b>Nov 14</b></td>
<td>Sequence-to-Sequence Learning</td>
<td>
</td>
</tr>

<tr>
<td><b>Nov 21</b></td>
<td>Attention Mechanisms and Neural Memories</td>
<td>
</td>
</tr>

<tr>
<td><b>Nov 28</b></td>
<td>Deep Reinforcement Learning</td>
<td>
</td>
</tr>

<tr>
<td><b>Dec 5</b></td>
<td>Deep Generative Models (Variational Auto-Encoders and Generative Adversarial Networks)</td>
<td>
Goodfellow et al. Ch. 20<br/>
Murphy, Ch. 28
</td>
</tr>

<tr>
<td><b>Dec 12</b></td>
<td>Final Projects I</td>
<td>
</td>
</tr>

<tr>
<td><b>Dec 19</b></td>
<td>Final Projects II</td>
<td>
</td>
</tr>


</table>
