Title: Home
URL: 
save_as: index.html


<table style="text-align: left; width: 100%;" border="0" cellpadding="2" cellspacing="20">
  <tbody>
    <tr>
      <td style="vertical-align: top; text-align: right;">
      <img style="width: 520px; height: 390px;" alt="Beijing, August 2015" src="{filename}/images/andre_beijing.jpg"></td>
      <td style="vertical-align: top;"><span style="font-weight: bold;"> Contact information:</span><br>
        <br>
        andre.t.martins AT tecnico DOT ulisboa DOT pt<br>
        <br>
        Instituto de Telecomunicacões<br>
        Torre Norte - Sala 9.07<br>
	Av. Rovisco Pais, 1
        1049-001 Lisboa - Portugal<br>
        <br>
        <span style="font-weight: bold;">Phone:</span> +351 218418454<br>
        <br>
        <a href="http://unbabel.com"><img style="border: 0px solid ; width: 80px;" alt="Unbabel logo" src="{filename}/images/unbabel-logo.png"></a>
        <a href="http://tecnico.ulisboa.pt"><img style="border: 0px solid ; width: 120px;" alt="IST logo" src="{filename}/images/IST-logo.png"></a><br>
        <a href="http://www.it.pt"><img style="border: 0px solid ; width: 180px;" alt="IT logo" src="{filename}/images/IT-logo.png"></a><br>
      </td>
    </tr>
  </tbody>
</table>

I am an Associate Professor at the Computer Science Department (DEI) and at the Electrical and Computer Engineering Department (DEEC) at [Instituto Superior Técnico](https://tecnico.ulisboa.pt). I am also the VP of AI Research at [Unbabel](http://unbabel.com) in [Lisbon](http://en.wikipedia.org/wiki/Lisbon), [Portugal](http://en.wikipedia.org/wiki/Portugal), and a Senior Researcher at the [Instituto de Telecomunicações](http://www.lx.it.pt), where I lead the [SARDINE Lab](https://andre-martins.github.io/pages/sardine.html). 

Until 2012, I was a PhD student in the joint CMU-Portugal program in Language Technologies, at [Carnegie Mellon University](http://www.cmu.edu) and at Instituto Superior Técnico, where I worked under the supervision of [Mário Figueiredo](http://www.lx.it.pt/~mtf), [Noah Smith](http://homes.cs.washington.edu/~nasmith/), [Pedro Aguiar](http://www.isr.ist.utl.pt/~aguiar) and [Eric Xing](http://www.cs.cmu.edu/~epxing). 

My research interests revolve around natural language processing and machine learning, more specifically sparse and structured transformations, uncertainty quantification, interpretability, and multimodal processing applied to machine translation, natural language generation, quality estimation, and evaluation. 
My research has been funded by a ERC Starting Grant (DeepSPIN) and Consolidator Grant (DECOLLAGE), among other grants, and has received several paper awards at ACL conferences. I co-founded and co-organize the [Lisbon Machine Learning School (LxMLS)](https://lxmls.it.pt). I am a Fellow of the [ELLIS society](https://ellis.eu) and a co-director of the [ELLIS Program in Natural Language Processing](https://ellis.eu/programs/natural-language-processing). I am a member of the [Lisbon Academy of Sciences](https://www.acad-ciencias.pt/) and of the Research & Innovation Advisory Group (RIAG) of the [EuroHPC Joint Undertaking](https://eurohpc-ju.europa.eu). 

Our work has been featured in the media:

- [Slator piece on "Adding Chocolate to Mint"](https://slator.com/unbabel-tackles-metric-bias-in-ai-translation/)  
- [Science Business piece on EuroLLM](https://sciencebusiness.net/news/r-d-funding/horizon-europe/artificial-intelligence-model-specialising-eu-languages-released)  
- [EuroLLM: Pioneering European Open Source AI](https://interoperable-europe.ec.europa.eu/collection/open-source-observatory-osor/news/eurollm-pioneering-european-open-source-ai)  
- [A EuroHPC Success Story: Speaking Freely with EuroLLM](https://eurohpc-ju.europa.eu/eurohpc-success-story-speaking-freely-eurollm_en)  
- [TechCrunch piece on Large AI Grand Challenge](https://techcrunch.com/2024/06/26/unbabel-among-first-ai-startups-to-win-millions-of-gpu-training-hours-on-eu-supercomputers/)  
- [Slator piece on our method to detect and correct MT hallucinations](https://slator.com/why-large-language-models-hallucinate-when-machine-translating-in-wild/)  

---

# Current Post-docs

- [Miguel Faria]() (Post-doc at IT, 2024-)
- [Giuseppe Attanasio](https://gattanasio.cc/) (Post-doc at IT, 2024-)
- [Marcos Treviso](http://mtreviso.github.io) (Post-doc at IT, 2023-)  
- [Ben Peters]() (Post-doc at IT, 2022-)

# Current PhD Students

- Beatriz Canaverde (PhD at IST, 2025-)  
- Miguel Ramos (PhD at IST, 2024-)  
- [Sonal Sannigrahi](https://sonalsannigrahi.github.io/) (PhD at IST, 2023-)  
- Emmanouil Zaranis (PhD at IST, 2023-)  
- Pavlo Vasylenko (PhD at IST, 2023-; co-supervised with [Marcos Treviso](https://mtreviso.github.io/))  
- Sophia Sklaviadis (PhD at IST, 2023-)  
- Margarida Campos (co-supervised with Mário Figueiredo from IST/UL)  
- [Saul Santos](https://ssantos97.github.io/) (co-supervised with [Daniel McNamee](https://fchampalimaud.org/research/groups/mcnamee) from Champalimaud Foundation)  
- Duarte Alves (PhD at IST, 2023-)  
- [António Farinhas](https://antonio-farinhas.github.io/) (PhD at IST, 2021-)  
- [Haau-Sing Li](https://lhaausing.github.io/) (ELLIS PhD at TU Darmstadt University, 2021-; co-advised with [Iryna Gurevych](https://www.informatik.tu-darmstadt.de/ukp/ukp_home/head_ukp/index.en.jsp))  
- [Patrick Fernandes](https://coderpat.github.io/) (PhD at CMU/IST, 2020-; co-advised with [Graham Neubig](http://www.phontron.com/))  
- [Nuno Guerreiro](https://nunonmg.github.io/) (PhD at IST, 2020-; co-advised with Pierre Colombo at University of Paris Saclay)  
- [Peiqin Lin](https://lpq29743.github.io/) (ELLIS PhD at LMU Munich, 2021-; co-supervised with Hinrich Schutze)  
- [Hugo Pitorro](https://pitorro.de/) (Researcher at IT, 2024-; co-supervised with [Marcos Treviso](https://mtreviso.github.io/))
- [Nuno Gonçalves]() (MSc Student at IST, 2024-; co-supervised with [Marcos Treviso](https://mtreviso.github.io/))   


# Alumni

- [Sweta Agrawal](https://sweta20.github.io/) (Post-doc at IT, 2024-2025, now Research Scientist at Google)
- [Chunchuan Lyu]() (Post-doc at IT, 2021-2022)  
- [Chryssa Zerva](https://www.linkedin.com/in/chryssa-zerva-7bb4a966) (Post-doc at IT, 2021-2023, now Assistant Professor at Instituto Superior Técnico)  
- [Vlad Niculae](http://vene.ro) (Post-doc at IT, 2018-2020, now Assistant Professor at University of Amsterdam)  
- [Erick Fonseca](http://www.nilc.icmc.usp.br/nilc/pessoas/erickrf) (Post-doc at IT, 2018-2020, now Data Scientist at Data Scientist at Kaufland)
- Taisiya Glushkova (PhD at IST, 2020-2024; co-advised with Chryssa Zerva at IST)
- [Marcos Treviso](http://mtreviso.github.io) (PhD at IST, 2019-2023, now post-doc at IT)  
- Ben Peters (PhD at IST, 2018-2023, now post-doc at IT)  
- Pedro Martins (PhD at IST, 2018-2022; co-advised with Zita Marinho from Google DeepMind, now at SAP)
- [Tsvetomila Mihaylova](https://tsvm.github.io) (PhD at IST, 2018-2022, co-advised with [Vlad Niculae](http://vene.ro), now at Aalto University)  
- [Gonçalo Correia](https://goncalomcorreia.github.io) (ELLIS PhD at IST, 2018-2022, co-advised with [Vlad Niculae](http://vene.ro), now at Priberam)
- [Gonçalo Faria](https://www.goncalofaria.com/) (Researcher at IT, 2023-2024; now PhD Student at the University of Washington)  
- Sameen Maruf (PhD at Monash University, 2016-: co-advised with [Reza Haffari](http://users.monash.edu.au/~gholamrh))
- [Zita Marinho](http://www.cs.cmu.edu/~zmarinho) (PhD at CMU/IST, 2013-2018; co-advised with [Geoff Gordon](http://www.cs.cmu.edu/~ggordon) and [Sidd Srinivasa](https://homes.cs.washington.edu/~siddh), now Head of Research at Priberam Labs)
- António Farinhas (MSc at IST, 2020)  
- Pedro Coelho (MSc at IST, 2020; co-advised with Christine Maroti at Unbabel)
- João Alves (MSc at IST, 2020; co-advised with Amin Farajian at Unbabel)
- João Moura (MSc at IST, 2020; co-advised with Fábio Kepler at Unbabel)
- Rita Costa (MSc at IST, 2020; co-advised with Luisa Coheur)
- [Nikita Nangia](https://woollysocks.github.io) (Summer Intern at IT, 2019)
- Michael Zhang (Research Intern at Unbabel, 2019)
- Daan van Stigt (Research Intern at Unbabel, 2019)
- [Telmo Pires](https://ai.google/research/people/telmo) (Research Intern at IT and Unbabel, 2017, now at Google AI)
- Pedro Ferreira (MSc at IST, 2017-)
- Chaitanya Malaviya (Research Intern at Unbabel, 2017, now MSc at CMU)
- [Julia Kreutzer](http://www.cl.uni-heidelberg.de/~kreutzer) (Research Intern at Unbabel, 2016, now PhD student at Heidelberg University)
- [Daniel Ferreira](https://www.nt.tuwien.ac.at/about-us/staff/daniel-luis-cavaco-ferreira) (MSc at IST, 2015; co-advised with Mariana Almeida; now PhD at Vienna Technical University)
- [Renato Negrinho](https://www.cs.cmu.edu/~negrinho) (Research Intern at Priberam, 2014, now PhD at CMU)
- Vasco Marcelino (MSc at IST, 2014; co-advised with Miguel Almeida)
- Marta Quintão (MSc at IST, 2014)
- Pedro Ramos (MSc at IST, 2013)

---

# News

* I presented ["Open & Multilingual LLMs for Europe"]({filename}/docs/EuroHPCSummit2025.pdf) in a discussion panel at [EuroHPC Summit](https://www.eurohpcsummit.eu/) in Krakow about the AI Factories. There I covered some of our recent successes with CroissantLLM, TowerLLM, EuroLLM, and EuroBERT.  
* New [EuroLLM website](https://eurollm.io)! We trained two LLMs from scratch, [EuroLLM-1.7B](https://huggingface.co/utter-project/EuroLLM-1.7B) and [EuroLLM-9B](https://huggingface.co/utter-project/EuroLLM-9B), using the European supercomputing infrastructure (EuroHPC). These models support 35 languages (including all 24 EU official languages). They were released fully open and are [among the best in several benchmarks](https://huggingface.co/blog/eurollm-team/eurollm-9b). They have 300k+ downloads so far! This was done in collaboration with Instituto Superior Técnico, Instituto de Telecomunicações, Unbabel, The University of Edinburgh, CentraleSupélec among others and was recently featured as [a success story at EuroHPC](https://eurohpc-ju.europa.eu/eurohpc-success-story-speaking-freely-eurollm_en). Larger and more powerful models are on the making now!  
* I gave recent talks at Cornell Tech ["Quality-Aware Generation: Reranking Laws and Insights from Communication Theory"](https://events.cornell.edu/event/lmss-cornell-tech-andre-f-t-martins-tecnico-lisboa), MIT ("Dynamic Sparsity for Machine Learning"), and EPFL ["xCOMET, Tower, EuroLLM: Open & Multilingual LLMs for Europe"](https://memento.epfl.ch/event/xcomet-tower-eurollm-open-multilingual-llms-for-eu/).  
* I presented a tutorial at NeurIPS 2024 with [Edoardo Ponti](https://ducdauge.github.io/) on [Dynamic Sparsity in Machine Learning: Routing Information through Neural Pathways](https://dynamic-sparsity.github.io/). Check it out! We have lots of materials, including slides and Jupyter notebooks.     
* We released TowerLLM 7B and 13B: multilingual LLMs for translation-related tasks. Check our [Tower Collection](https://huggingface.co/collections/Unbabel/tower-659eaedfe36e6dd29eb1805c) at Hugging Face. These models and datasets are now widely used by the community (200k+ downloads so far). We presented this work as a spotlight paper at COLM 2024. 
* We participated for the first time in the WMT 2024 shared task on General Translation -- and we were the best participating system, with the best results in 8 out of 11 languages! (Bonus: we also won the Biomedical and the Chat Translation task!) Fruit of this work, Unbabel launched [Widn.Ai](https://www.widn.ai/) -- the highest quality MT engine which can be personalized with instructions and used as an API. Try it out!  
* We built [xCOMET](https://huggingface.co/collections/Unbabel/xcomet-659eca973b3be2ae4ac023bb), a state-of-the-art interpretable model for MT evaluation and quality estimation. Give it a try!  It was published in TACL and presented at ACL 2024.  
* We were one of the 4 winning projects of the Large AI Grand Challenge grant (AI Boost), a highly competitive grant which comes with 2M GPU hours: https://lnkd.in/ghYTVaDb. We are using this allocation to train a mixture-of-experts version of Tower.
* In 2024, our team presented 20+ papers in top conferences (including NeurIPS, ICLR, ICML, COLM, TACL, EMNLP, COLM, ICML, ...). We had spotlight/oral papers at ICML, NeurIPS, and COLM. We presented in several keynote talks in workshops and other events.  
* I am Program Co-Chair of ACL 2024.  
* **Great news: I got an ERC (European Research Council) Consolidator Grant on "Deep Cognition Learning for Language Generation (DECOLLAGE)". [I am now looking for Post-Doc and PhD Students](pages/jobs.html).**
* I gave a keynote talk at the [SEPLN 2022](https://sepln2022.grupolys.org/) conference.
* I gave a keynote at the Mercury Machine Learning Lab ([MMLL](https://icai.ai/mercury-machine-learning-lab/)) seminar series. I talked about how to go from sparse modeling to sparse communication. Check the video [here](https://www.youtube.com/watch?v=UFsCAr4kIc0&list=PLTg_E6ob657XajMOqJ4HxfQcv49f8xD_Z&t=8s)!
* I am co-organizing LxMLS 2022 (Lisbon Machine Learning School), back to in-person this year! See [here](http://lxmls.it.pt) for details!
* I am SAC for ACL 2022 and AC for NeurIPS 2022. 
* We have new papers accepted at CLeaR 2022, ICLR 2022, ACL 2022, NAACL 2022, ICML 2022.
* I gave a keynote talk at the [TRITON](https://triton-conference.org/) conference.
* I gave a keynote talk at [TALN](https://talnrecital2021.inria.fr) where I presented some of the work we did in the [DeepSPIN](https://deep-spin.github.io/) project. [Here](docs/taln2021.pdf) are the slides.
* I am co-organizing LxMLS 2021 (Lisbon Machine Learning School), which will be a fully remote school this year. See [here](http://lxmls.it.pt) for details!
* We have new papers accepted at NAACL 2021 and ACL 2021.
* I am AC for NAACL 2021, NeurIPS 2021, EMNLP 2021, ICLR 2022.
* I am Action Editor for TACL.
* I am co-directing the [ELLIS NLP program](https://ellis.eu/programs/natural-language-processing) with [Iryna Guleyvich](https://www.informatik.tu-darmstadt.de/ukp/ukp_home/staff_ukp/prof_dr_iryna_gurevych/index.en.jsp) and [Ivan Titov](http://ivan-titov.org/), with an amazing list of fellows and scholars!
* We're launching [LUMLIS](https://lumlis.tecnico.ulisboa.pt), the Lisbon unit of the [ELLIS](https://ellis.eu) network!
* We have new papers accepted at EMNLP 2020 and NeurIPS 2020.
* I am SAC for EACL 2021 and AC for ICLR 2021.
* **We have an opening for a Post-Doc position in the DeepSPIN project.** See [here](pages/jobs.html) for details!
* We have new papers accepted at ACL 2020, ICML 2020, and EAMT 2020.
* We have a new [JMLR paper](http://jmlr.csail.mit.edu/papers/volume21/19-021/19-021.pdf) on Learning with Fenchel-Young Losses (work done with collaboration with [Mathieu Blondel](http://mblondel.org)).
* I am AC for NeurIPS 2020 and EMNLP 2020.
* I was SAC for ACL 2020 and AC for ICLR 2020.
* We presented a [tutorial on Latent Structure Models for NLP](https://deep-spin.github.io/tutorial/) at ACL 2019 in Florence.  
* I am giving an invited talk at the [First EurNLP Summit](https://www.eurnlp.org) in London.  
* I am giving invited talks at 3 Summer schools: [LxMLS 2019](http://lxmls.it.pt) in Lisbon, [AthNLP 2019](http://athnlp.iit.demokritos.gr) in Athens, and [MLRS 2019](https://www.mlrs.ai) in Bangkok. 
* I was part of a discussion panel at Culturgest about AI. Here's the [video](https://www.culturgest.pt/pt/programacao/ana-paiva-andre-martins-e-arlindo-oliveira-especulacoes) (in Portuguese).
* We have new papers accepted at AISTATS, NAACL, ACL, and EMNLP 2019.
* Unbabel won the WMT 2019 Shared Task on Quality Estimation! Check the results [here](http://www.statmt.org/wmt19/qe-results.html)!
* We received the **best system demo paper award** for OpenKiwi, a Pytorch-based software toolkit for translation quality estimation. Check the [repo](https://github.com/Unbabel/OpenKiwi) and the [demo paper](https://arxiv.org/abs/1902.08646) at ACL 2019!   
* We have a new EMNLP paper where we propose **SparseMAP** to build dynamic computation graphs via sparse latent structure.
* I gave an invited talk in the [ACL 2018 Workshop on Neural Machine Translation and Generation](https://sites.google.com/site/wnmt18). Here are the [slides](https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnx3bm10MTh8Z3g6NzM2ZWNhMTk2MTdlODQ2Yw).
* We have a new [ICML paper](http://proceedings.mlr.press/v80/niculae18a.html) where we propose *SparseMAP* as a new inference procedure for sparse structured prediction (work done in collaboration with [Vlad Niculae](http://vene.ro), [Mathieu Blondel](http://mblondel.org), and [Claire Cardie](http://www.cs.cornell.edu/home/cardie)).
* We have a new [ACL short paper](http://aclweb.org/anthology/P18-2059) where we use new forms of sparse and constrained attention within neural machine translation (work done in collaboration with Chaitanya Malaviya and Pedro Ferreira).
* **Great news: I got an ERC (European Research Council) Starting Grant on "Deep Structured Prediction in Natural Language Processing (DeepSPIN)". [I am now looking for Post-Doc and PhD Students](pages/jobs.html).**
* I'm giving an invited talk in the [EMNLP 2017 Workshop on Structured Prediction for NLP](http://structuredprediction.github.io/EMNLP17).
* I gave an invited talk in the [ICML 2017 Workshop on Learning to Generate Natural Language](https://sites.google.com/site/langgen17) and co-organized the [ICML 2017 Workshop on Deep Structured Prediction](https://deepstruct.github.io/ICML17).
* [Our paper on end-to-end differentiable neural-easy first decoders]({filename}/docs/emnlp2017_final.pdf) made it to EMNLP 2017!
* We have a new [TACL paper](https://transacl.org/ojs/index.php/tacl/article/view/1113) where we further improve our quality estimation system by using automatic post-editing as a auxiliary task.
* Our word-level quality estimation system achieved the top score in the First Conference on Machine Translation ([WMT 2016](http://www.statmt.org/wmt16)). Check the paper [here](https://www.aclweb.org/anthology/W/W16/W16-2387.pdf).
* [Our paper on sparsemax](http://jmlr.org/proceedings/papers/v48/martins16.pdf) was accepted at [ICML 2016](http://icml.cc/2016). It's a sparse, differentiable, alternative to the softmax activation.
* I joined [Unbabel](http://unbabel.com)! We do AI powered human quality translation.
* I presented a tutorial on "Linear Programming Decoders in NLP" at EMNLP 2014, which covers integer linear programming, message-passing algorithms, and dual decomposition. The slides are available [here]({filename}/docs/emnlp2014tutorial.pdf) and the videos here ([part 1](http://www.youtube.com/watch?v=JySNOVdYNgc), [part 2](http://www.youtube.com/watch?v=BvmzioMGK2Q)).
* Our [TurboSemanticParser](http://labs.priberam.com/Resources/TurboSemanticParser) system achieved the top score in the open track of the [SemEval 2014 task on "Broad-Coverage Semantic Dependency Parsing."](http://alt.qcri.org/semeval2014/task8/) Check the paper [here]({filename}/docs/semeval2014_task8.pdf).
* I presented a tutorial on "Structured Sparsity in NLP" at EACL 2014. The slides are available [here]({filename}/docs/eacl2014tutorial.pdf) (see [here]({filename}/docs/naacl2012tutorial.pdf) for an older version of the same tutorial presented at NAACL 2012).
* I received the Portuguese [IBM Scientific Prize](http://www-05.ibm.com/pt/pc/crct) for my work in structured prediction and "Turbo Parsing."
* I successfully defended my [thesis]({filename}/docs/thesis.pdf) in May 11, 2012, at CMU!